{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees is one of the basic supervised learning algorithm which can be used for both regression and classification problems. We focus specifically on the classification problem only. In this problem, the aim is to build a tree starting from root at the top to the leaves in the bottom with specific decision as each nodes (questions that can separate the dataset into two halves). The whole algorithm can be divided into some steps:\n",
    "    1. Generating list of valid questions\n",
    "    2. Using some metric to measure the effectiveness of each question\n",
    "    3. Choosing the optimal question and splitting the dataset\n",
    "    4. Iterating on the subsets, stopping once no split is possible (homogeneous class) or some depth criteria\n",
    "This is a very simple approach to learn a decision tree model. To measure the effectiveness of a question we use the concepts of Gini Impurity and Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Impuity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measures the chance of a randomly picked sample being incorrectly labelled if the label is randomly picked from the given label distribution in the set. This can be written as:\n",
    "$$I_G(p) = \\sum_{i=1}^{J}p_i(1 - p_i) = 1 - \\sum_{i=1}^{J}p_i^2$$\n",
    "where $p_i$ is the probability of $i^{th}$ label and $J$ is the class size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the change in the entropy from previous state to the current state. More formally,\n",
    "$$IG(T, a) = H(T) - H(T|a)$$\n",
    "where $H(T) = -\\sum p_i log_2(p_i)$ is the entropy of the system in original state and $H(T|a)$ is the weighted average entropy of the system after action $a$.\n",
    "This can be defined using Gini Impurity as well. Given we have sample $S$ and action/question $a$ generates $V$ subsets $S_i$. Then, information gain is given by:\n",
    "$$IG(S, a) = I_G(S) - \\sum_{i=1}^{V}\\frac{|S_i|}{|S|}I_G(S_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating questions and splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate valid questions we need to iterate over all features to get the unique values, setup equality/inequality questions about them. For numerical values we set greater than equal to condition and for categorical values we set equality condition as question criteria. Based on this, the dataset is split into true set and false set and Information Gain is estimated based on Gini Impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define valid questions given a dataset, functions to calculate gini impurity, information gain for given splits and then decide on the optimal split. Then a function can use all these facilities to build a decision tree from a given decision tree. The dataset is defined as feature matrix $X$ and the class labels/target vector $Y$. For defining the questions, we need the specific feature and the numeric/text value i.e. something like (age, 5) which means that the question formed is 'Is age greater than or equal to 5'. Similarly, for text based questions it could mean that (gender, female) means the question 'Is the gender equal to female'. The dataset split happens such that the true cases go to left and the false cases to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class TreeNode:\n",
    "    \n",
    "    def __init__(self, nodeType='decision', attribute=None, left=None, right=None):\n",
    "        self.nodeType = nodeType    # Node can be of type 'decision' or 'leaf'\n",
    "        self.attribute = attribute  # For decision -> question(of type list)\n",
    "                                    # For leaf -> class(of type string/counter)\n",
    "        self.left = left            # Reference to left subtree\n",
    "        self.right = right          # Pointer to right subtree\n",
    "\n",
    "    def printTree(self):\n",
    "        print(self.nodeType, self.attribute)\n",
    "        if self.nodeType == 'leaf':\n",
    "            return\n",
    "        if self.left is None:\n",
    "            print('No leaf in left')\n",
    "        else:\n",
    "            self.left.printTree()\n",
    "        if self.right is None:\n",
    "            print('No leaf in right')\n",
    "        else:\n",
    "            self.right.printTree()\n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, metric='gini', minLeaves=2):\n",
    "        self.metric = metric         # the metric used for quantifying questions (only gini supported for now)\n",
    "        self.minLeaves = minLeaves   # the minimum size of dataset required for splitting a node\n",
    "        self.rootNode = None         # Internal pointer to the root node of the decision tree learned\n",
    "    \n",
    "    def getRootNode(self):\n",
    "        return self.rootNode\n",
    "    \n",
    "    def __countClassLabel(self, Y):\n",
    "        return Counter(Y)\n",
    "    \n",
    "    def __isNumeric(self, val):\n",
    "        return isinstance(val, int) or isinstance(val, float)\n",
    "    \n",
    "    def __giniImpurity(self, Y):\n",
    "        LabelCounts = self.__countClassLabel(Y)\n",
    "        giniImpurity = 1\n",
    "        for label in LabelCounts:\n",
    "            giniImpurity -= (LabelCounts[label] / float(len(Y))) ** 2\n",
    "        return giniImpurity\n",
    "    \n",
    "    def __informationGain(self, leftSplitY, rightSplitY, parentGiniImpurity):\n",
    "        weightLeft = len(leftSplitY) / float(len(leftSplitY) + len(rightSplitY))\n",
    "        return parentGiniImpurity - weightLeft * self.__giniImpurity(leftSplitY) \\\n",
    "                    - (1 - weightLeft) * self.__giniImpurity(rightSplitY)\n",
    "    \n",
    "    def __uniqueLabels(self, X, featureIndex):\n",
    "        return set([x[featureIndex] for x in X])   #select all rows for given feature index\n",
    "    \n",
    "    def __createQuestions(self, X):\n",
    "        # generate the possbile valid questions in the format:\n",
    "        # [feature index, value] i.e. [0, 5] => 'age greater than equal to 5' if\n",
    "        # 0th feature is age.\n",
    "        validQuestions = []\n",
    "        for index in range(len(X[0])):\n",
    "            uniqueVals = self.__uniqueLabels(X, index)\n",
    "            validQuestions.extend([index, val] for val in uniqueVals)\n",
    "        return validQuestions\n",
    "    \n",
    "    def __match(self, x, question):\n",
    "        # x is a row from feature matrix X\n",
    "        [featureIndex, value] = question\n",
    "        if self.__isNumeric(value):\n",
    "            return x[featureIndex] >= value\n",
    "        else:\n",
    "            return x[featureIndex] == value\n",
    "\n",
    "    def __splitDataset(self, X, Y, question):\n",
    "        # question is an item from the list of valid questions of the format [feature index, value]\n",
    "        leftX = []; rightX = []; leftY = []; rightY = []\n",
    "        for idx, row in enumerate(X):\n",
    "            if self.__match(row, question):\n",
    "                leftX.append(row)\n",
    "                leftY.append(Y[idx])\n",
    "            else:\n",
    "                rightX.append(row)\n",
    "                rightY.append(Y[idx])\n",
    "        return [leftX, leftY, rightX, rightY]\n",
    "    \n",
    "    def __bestQuestion(self, X, Y):\n",
    "        # find the optimal question for a given dataset. Also, return the corresponding gain\n",
    "        bestGain = 0\n",
    "        bestQuestion = []\n",
    "        allQuestions = self.__createQuestions(X)\n",
    "        parentGini = self.__giniImpurity(Y)\n",
    "        for question in allQuestions:\n",
    "            [leftX, leftY, rightX, rightY] = self.__splitDataset(X, Y, question)\n",
    "            gain = self.__informationGain(leftY, rightY, parentGini)\n",
    "            if gain > bestGain:\n",
    "                bestGain = gain\n",
    "                bestQuestion = question\n",
    "        return [bestGain, bestQuestion]\n",
    "    \n",
    "    def __fitHelper(self, X, Y):\n",
    "        [gain, question] = self.__bestQuestion(X, Y)\n",
    "        \n",
    "        # if no gain or minLeaves reached, this means this is a leaf node. For now, class is predicted\n",
    "        # as the majority in the split.\n",
    "        if gain == 0 or len(Y) <= self.minLeaves:\n",
    "            # classValue = self.__countClassLabel(Y).most_common(1)[0][0]\n",
    "            classValue = self.__countClassLabel(Y)\n",
    "            return TreeNode(nodeType='leaf', attribute=classValue)\n",
    "        \n",
    "        [leftX, leftY, rightX, rightY] = self.__splitDataset(X, Y, question)\n",
    "        leftNode = self.__fitHelper(leftX, leftY)\n",
    "        rightNode = self.__fitHelper(rightX, rightY)\n",
    "        \n",
    "        return TreeNode(nodeType='decision', attribute=question, left=leftNode, right=rightNode)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.rootNode = self.__fitHelper(X, Y)\n",
    "    \n",
    "    def printDecisionTree(self):\n",
    "        if self.rootNode is None:\n",
    "            print('Cannot print tree with none root type!!')\n",
    "        self.rootNode.printTree()\n",
    "    \n",
    "    def __predictHelper(self, x, currentNode):\n",
    "        if currentNode.nodeType == 'leaf':\n",
    "            # At this point, for now, just return the most obvious choice for class\n",
    "            return currentNode.attribute.most_common(1)[0][0]\n",
    "        \n",
    "        if self.__match(x, currentNode.attribute):\n",
    "            return self.__predictHelper(x, currentNode.left)\n",
    "        else:\n",
    "            return self.__predictHelper(x, currentNode.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self.__predictHelper(x, self.rootNode) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preorder traversal of the decision tree: format<nodetype attribute>\n",
      "decision [0, 'Red']\n",
      "leaf Counter({'Grape': 2})\n",
      "decision [1, 4]\n",
      "leaf Counter({'Apple': 2})\n",
      "decision [0, 'Yellow']\n",
      "leaf Counter({'Apple': 1, 'Lemon': 1})\n",
      "leaf Counter({'Apple': 1})\n",
      "\n",
      "Predicting on the same set:\n",
      "['Apple', 'Apple', 'Grape', 'Grape', 'Apple', 'Apple', 'Apple']\n"
     ]
    }
   ],
   "source": [
    "X = [['Green', 3],\n",
    "    ['Yellow', 3],\n",
    "    ['Red', 1],\n",
    "    ['Red', 1],\n",
    "    ['Yellow', 3],\n",
    "    ['Yellow', 4],\n",
    "    ['Yellow', 5]]\n",
    "#NOTE: If using numpy array -> dtype object is required to maintain the string/integer types as is.\n",
    "#If not mentioned, will convert int to string\n",
    "\n",
    "Y = ['Apple', 'Apple', 'Grape', 'Grape', 'Lemon', 'Apple', 'Apple']\n",
    "\n",
    "D = DecisionTree()\n",
    "D.fit(X, Y)\n",
    "print('Preorder traversal of the decision tree: format<nodetype attribute>')\n",
    "D.printDecisionTree()\n",
    "print('\\nPredicting on the same set:')\n",
    "print(D.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
